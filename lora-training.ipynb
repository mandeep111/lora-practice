{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f0038f",
   "metadata": {},
   "source": [
    "Connect To Google Drive.  In this example dataset is a zip file in the specified location. (/content/drive/MyDrive/datasets/dataset.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a22db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!unzip -o \"/content/drive/MyDrive/datasets/dataset.zip\" -d \"/content/lora_dataset\"\n",
    "\n",
    "# Mounted at /content/drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68cd55b",
   "metadata": {},
   "source": [
    "Create Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a657f6",
   "metadata": {},
   "source": [
    "!pip install virtualenv\n",
    "!virtualenv loraenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e368dea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# every cell that runs code must start with this:\n",
    "import sys\n",
    "sys.path.insert(0, '/content/loraenv/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d2370",
   "metadata": {},
   "source": [
    "Install Required Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e358f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Clean slate (optional but recommended on fresh runtime)\n",
    "!pip uninstall -q -y diffusers transformers accelerate peft datasets torch torchvision torchaudio sentence-transformers\n",
    "\n",
    "# 2. Install PyTorch with correct CUDA (Colab uses CUDA 12.x)\n",
    "!pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 3. Install bleeding-edge diffusers + transformers from GitHub (required for the latest train_lora_sdxl.py)\n",
    "!pip install -q git+https://github.com/huggingface/diffusers.git\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q git+https://github.com/huggingface/accelerate.git\n",
    "!wget -O train_lora_sdxl.py https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/train_text_to_image_lora_sdxl.py\n",
    "\n",
    "# 4. Remaining core dependencies\n",
    "!pip install -q peft bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb726adf",
   "metadata": {},
   "source": [
    "Check for any broken dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c651d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!/content/loraenv/bin/pip check\n",
    "\n",
    "# No broken requirements found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f6ed6",
   "metadata": {},
   "source": [
    "Generate Caption for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323cfb62",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dataset_dir = \"/content/lora_dataset\"\n",
    "\n",
    "metadata_path = os.path.join(dataset_dir, \"metadata.jsonl\")\n",
    "\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    for img_file in os.listdir(dataset_dir):\n",
    "        if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "            txt_file = base_name + \".txt\"\n",
    "            txt_path = os.path.join(dataset_dir, txt_file)\n",
    "            \n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, \"r\") as tf:\n",
    "                    caption = tf.read().strip()\n",
    "            else:\n",
    "                caption = \"\"  # fallback empty caption\n",
    "            \n",
    "            entry = {\"file_name\": img_file, \"text\": caption}\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Created {metadata_path} with captions from .txt files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd8ed5",
   "metadata": {},
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193507f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# accelerate command\n",
    "!accelerate launch /content/train_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "  --train_data_dir=\"/content/lora_dataset\" \\\n",
    "  --output_dir=\"/content/lora_output\" \\\n",
    "  --resolution=1024 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=1200 \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --gradient_checkpointing \\\n",
    "  --use_8bit_adam \\\n",
    "  --train_text_encoder \\\n",
    "  --seed=42 \\\n",
    "  --validation_prompt=\"detailed face, masterpiece, best quality\" \\\n",
    "  --validation_epochs=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a193f",
   "metadata": {},
   "source": [
    "Save Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e6a45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "\n",
    "checkpoints = sorted(glob.glob(\"/content/lora_output/checkpoint-*\"), key=os.path.getmtime)\n",
    "last_ckpt = checkpoints[-1]\n",
    "print(\"Last checkpoint:\", last_ckpt)\n",
    "\n",
    "lora_path = os.path.join(last_ckpt, \"pytorch_lora_weights.safetensors\")\n",
    "final_lora = \"/content/lora_output/trained_lora.safetensors\"\n",
    "\n",
    "shutil.copy(lora_path, final_lora)\n",
    "print(\"Saved final LoRA:\", final_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3884b",
   "metadata": {},
   "source": [
    "Upload trained LoRA to Hugging Face. You'll need to create hugging face token first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970041b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi, upload_file\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752d3c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "repo_id = \"your-username/my-first-lora\"   # change to your namespace and repo name\n",
    "api.create_repo(repo_id, private=False, exist_ok=True)\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj=final_lora,\n",
    "    path_in_repo=\"trained_lora.safetensors\",\n",
    "    repo_id=repo_id\n",
    ")\n",
    "\n",
    "print(\"Uploaded Successfully:\", repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aad4ba",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
